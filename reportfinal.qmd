---
title: "Pedestrian Crossings Data Analysis Report (ETC5242 Assignment 2)"
author: "Jyovika Aswale, Sia Chawla, Siddhi Jadhav"
format: html
editor: visual
---

```{r}
#| label: setup
#| include: false
#| echo: false
#| message: false
#| warning: false

knitr::opts_chunk$set(fig.align = "center", fig.width = 8, fig.height = 5, dpi = 150)

library(boot)
library(MASS)
library(tidyverse)
library(knitr)
library(kableExtra)
library(broom)
library(purrr)
library(scales)

set.seed(355435)  

B <- 5000

cross_cols <- c(
  "Flinders Street" = "#766153",
  "QV Melbourne" = "#718f94",
  "Southern Cross"  = "#717744")
```

## Task 1 - Distribution and Models

```{r}
pedestrian_df <- read.csv(here::here("data/pedestrians.csv"))
```

### Purpose and approach

The aim was to describe how lunchtime pedestrian counts vary at the three crossings (Flinders Street, Southern Cross, QV Melbourne) and to fit probability models that summarise these patterns for design and planning. I began with exploratory graphics (histograms/densities and boxplots; see Figures X–Y) and computed descriptive statistics per site (mean, median, sd, min–max, variance; see Table A). These summaries show QV Melbourne has the largest flows and greater day-to-day variation, while Flinders Street and Southern Cross have lower, tighter distributions.

```{r, EDA_plots}
pedestrian_long <- pedestrian_df %>%
  mutate(obs = row_number()) %>%
  pivot_longer(-obs, names_to = "crossing", values_to = "count") %>%
  mutate(crossing = recode(
    crossing,
    "flinders_street" = "Flinders Street",
    "qv_melbourne"    = "QV Melbourne",
    "southern_cross"  = "Southern Cross")) %>%
  mutate(crossing = factor(
    crossing,
    levels = c("Southern Cross", "Flinders Street", "QV Melbourne")))

```
::: {.columns}
::: {.column width="50%"}

```{r,boxplot}
ggplot(pedestrian_long, aes(x = crossing, y = count, fill = crossing)) +
  geom_boxplot(outlier.shape = NA, alpha = 0.8) +
  scale_fill_manual(values = cross_cols) + 
  labs(title = "Pedestrian counts by crossing", x = NULL, y = "Count per interval") +
  theme_minimal() + 
  theme(legend.position = "none")
```
:::
::: {.column width="50%"}

```{r, density_plot}
ggplot(pedestrian_long, aes(x = count, fill = crossing)) +
  geom_density(alpha = 0.35) +
  facet_wrap(~ crossing, scales = "fixed") +
  scale_fill_manual(values = cross_cols) + 
  labs(title = "Distribution (density) per crossing", x = "Count", y = "Density") +
  theme_minimal() +
  theme(legend.position = "none")
```
:::
:::

As shown in @fig:EDA_plots, pedestrian activity differs clearly across the three crossings. The boxplots indicate that QV Melbourne consistently records the highest pedestrian counts, with a median around 2.3–2.5 k per interval and the widest spread-showing both heavy usage and large fluctuations. Southern Cross and Flinders Street have lower medians (roughly 900–1,000 and 800–900 respectively) and narrower boxes, suggesting steadier, lighter flows.

The density curves in @fig:EDA_plots confirm that all three distributions are unimodal and right-skewed, reflecting occasional high-traffic bursts. QV Melbourne’s distribution sits furthest to the right, while Southern Cross and Flinders overlap slightly, with Flinders trending marginally lower.

Overall, @fig:EDA_plots highlights that QV Melbourne is the busiest and most variable crossing-prioritise it for any infrastructure or timing upgrades-while Southern Cross and Flinders Street appear relatively stable and would need only minor adjustments.

```{r, EDA_summaries, tbl:crossing_summary}
pedestrian_long %>%
  group_by(crossing) %>%
  summarise(
    n     = n(),
    mean  = mean(count),
    median= median(count),
    sd    = sd(count),
    min   = min(count),
    max   = max(count),
    iqr   = IQR(count),
    .groups = "drop"
) %>%
  kable(caption = "Younger vs Older: Average Per-capita Online Leisure Minutes",
        digits = 1, align = c("l","r")) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed", "responsive"),
    full_width = FALSE
  ) %>%
  row_spec(0, bold = TRUE, color = "#FFFFFF", background = "#598392") %>%
  column_spec(1, bold = TRUE, color = "#004D40")
```

Table @tbl:crossing_summary provides summary statistics for pedestrian counts at each crossing (n = 97 per site). Results align with the distributional patterns in @fig:EDA_plots.

**QV Melbourne.** Mean 2,336.7 and median 2,310 indicate the highest sustained demand. Dispersion is largest in absolute terms (sd ≈305, IQR 462; range 1,773–3,122), confirming substantial variability and frequent high-traffic intervals.

**Southern Cross.** Central tendency is lower (mean 941.8, median 955) with comparatively tight spread (sd ≈133, IQR 143; range 494–1,256). This suggests a steady, moderate flow with fewer extreme peaks.

**Flinders Street.** Typical load is slightly below Southern Cross (mean 893.0, median 887), but variability through the middle of the distribution is higher (IQR 202, sd ≈148; range 590–1,282), indicating more fluctuation around a lower centre.

Skew. For all sites, the mean exceeds the median, consistent with right-skew seen in @fig:EDA_plots, i.e., occasional upper-tail surges.

-   **Implications for design and operations.**

    -   Prioritise QV Melbourne for capacity and signal-timing upgrades; plan to the upper percentiles rather than the mean.

    -   Southern Cross appears operationally stable; targeted, lighter interventions are likely sufficient.

    -   Flinders Street runs slightly lighter than Southern Cross but shows greater mid-spread variability; monitor and manage for intermittent congestion.

### Model fitting and resulting estimates

```{r, MLE}
# Southern Cross
sc <- pedestrian_df$southern_cross
fit_sc_norm  <- fitdistr(sc, "normal")
fit_sc_logn  <- fitdistr(sc, "lognormal")
fit_sc_gamma <- fitdistr(sc, "gamma")   

sc_models <- tibble(
  crossing = "Southern Cross",
  model    = c("normal","lognormal","gamma"),
  logLik   = c(logLik(fit_sc_norm)[1],  logLik(fit_sc_logn)[1],  logLik(fit_sc_gamma)[1]),
  AIC      = c(AIC(fit_sc_norm),        AIC(fit_sc_logn),        AIC(fit_sc_gamma)),

  mean_est = c(
    unname(fit_sc_norm$estimate["mean"]),
    exp(fit_sc_logn$estimate["meanlog"] + 0.5*fit_sc_logn$estimate["sdlog"]^2),
    unname(fit_sc_gamma$estimate["shape"] / fit_sc_gamma$estimate["rate"])),
  
  sd_est   = c(
    unname(fit_sc_norm$estimate["sd"]),
    sqrt((exp(fit_sc_logn$estimate["sdlog"]^2) - 1) *
         exp(2*fit_sc_logn$estimate["meanlog"] + fit_sc_logn$estimate["sdlog"]^2)),
    sqrt(unname(fit_sc_gamma$estimate["shape"]) / (unname(fit_sc_gamma$estimate["rate"])^2))))

# Flinders Street
fl <- pedestrian_df$flinders_street
fit_fl_norm  <- fitdistr(fl, "normal")
fit_fl_logn  <- fitdistr(fl, "lognormal")
fit_fl_gamma <- fitdistr(fl, "gamma")

fl_models <- tibble(
  crossing = "Flinders Street",
  model    = c("normal","lognormal","gamma"),
  logLik   = c(logLik(fit_fl_norm)[1],  logLik(fit_fl_logn)[1],  logLik(fit_fl_gamma)[1]),
  AIC      = c(AIC(fit_fl_norm),        AIC(fit_fl_logn),        AIC(fit_fl_gamma)),

  mean_est = c(
    unname(fit_fl_norm$estimate["mean"]),
    exp(fit_fl_logn$estimate["meanlog"] + 0.5*fit_fl_logn$estimate["sdlog"]^2),
    unname(fit_fl_gamma$estimate["shape"] / fit_fl_gamma$estimate["rate"])),
  
  sd_est   = c(
    unname(fit_fl_norm$estimate["sd"]),
    sqrt((exp(fit_fl_logn$estimate["sdlog"]^2) - 1) *
         exp(2*fit_fl_logn$estimate["meanlog"] + fit_fl_logn$estimate["sdlog"]^2)),
    sqrt(unname(fit_fl_gamma$estimate["shape"]) / (unname(fit_fl_gamma$estimate["rate"])^2))))

# QV Melbourne
qv <- pedestrian_df$qv_melbourne
fit_qv_norm  <- fitdistr(qv, "normal")
fit_qv_logn  <- fitdistr(qv, "lognormal")
fit_qv_gamma <- fitdistr(qv, "gamma")

qv_models <- tibble(
  crossing = "QV Melbourne",
  model    = c("normal","lognormal","gamma"),
  logLik   = c(logLik(fit_qv_norm)[1],  logLik(fit_qv_logn)[1],  logLik(fit_qv_gamma)[1]),
  AIC      = c(AIC(fit_qv_norm),        AIC(fit_qv_logn),        AIC(fit_qv_gamma)),

  mean_est = c(
    unname(fit_qv_norm$estimate["mean"]),
    exp(fit_qv_logn$estimate["meanlog"] + 0.5*fit_qv_logn$estimate["sdlog"]^2),
    unname(fit_qv_gamma$estimate["shape"] / fit_qv_gamma$estimate["rate"])),
  
  sd_est   = c(
    unname(fit_qv_norm$estimate["sd"]),
    sqrt((exp(fit_qv_logn$estimate["sdlog"]^2) - 1) *
         exp(2*fit_qv_logn$estimate["meanlog"] + fit_qv_logn$estimate["sdlog"]^2)),
    sqrt(unname(fit_qv_gamma$estimate["shape"]) / (unname(fit_qv_gamma$estimate["rate"])^2))))

mle_fits <- bind_rows(sc_models, fl_models, qv_models) %>%
  arrange(crossing, AIC)

mle_fits  %>%
  kable(
    caption = "Younger vs Older: Average Per-capita Online Leisure Minutes",
    digits = 1,
    align = rep("c")  
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed", "responsive"),
    full_width = FALSE,
    position = "center"
  ) %>%
  row_spec(0, bold = TRUE, color = "#FFFFFF", background = "#598392") %>%
  column_spec(1, bold = TRUE, color = "#004D40")
```

Guided by the shapes in the graphics and Q–Q diagnostics, I fitted simple parametric models via maximum likelihood. Southern Cross is approximately symmetric on the original scale, so a **Normal** model was used. Flinders Street and QV Melbourne display mild right-skew and near-normality on the log scale, so **Lognormal** models were used. The resulting parameter estimates (estimate ± SE) were:

-   **Flinders Street (Lognormal):** meanlog = 6.7813 (0.0166), sdlog = 0.1633 (0.0117)\
-   **Southern Cross (Normal):** mean = 941.78 (13.44), sd = 132.34 (9.50)\
-   **QV Melbourne (Lognormal):** meanlog = 7.7482 (0.0130), sdlog = 0.1283 (0.0092)

These are the fitted model parameters that mathematically characterise each site’s distribution.

```{r, qqplots}
par(mfrow = c(1,3), mar = c(4,4,2,1))

## Southern Cross 
x <- sort(sc); n <- length(x); p <- (seq_len(n) - 0.5) / n
qN <- qnorm(p,  mean = fit_sc_norm$estimate["mean"],    sd = fit_sc_norm$estimate["sd"])
qL <- qlnorm(p, meanlog = fit_sc_logn$estimate["meanlog"], sdlog = fit_sc_logn$estimate["sdlog"])
qG <- qgamma(p, shape = fit_sc_gamma$estimate["shape"], rate = fit_sc_gamma$estimate["rate"])

plot(qN, x, xlab = "Theoretical quantiles", ylab = "Empirical quantiles",
     main = "Southern Cross", pch = 16, col = "#8F250C", cex = 0.8)
points(qL, x, pch = 16, col = "#9cb380", cex = 0.8)
points(qG, x, pch = 16, col = "#124559", cex = 0.8)
abline(0, 1, lty = 2)
legend("topleft", legend = c("Normal", "Lognormal", "Gamma"),
       col = c("#8F250C", "#9cb380", "#124559"), pch = 16, bty = "n", cex = 0.8)

# Flinders Street
x <- sort(fl); n <- length(x); p <- (seq_len(n) - 0.5) / n
qN <- qnorm(p,  mean = fit_fl_norm$estimate["mean"],    sd = fit_fl_norm$estimate["sd"])
qL <- qlnorm(p, meanlog = fit_fl_logn$estimate["meanlog"], sdlog = fit_fl_logn$estimate["sdlog"])
qG <- qgamma(p, shape = fit_fl_gamma$estimate["shape"], rate = fit_fl_gamma$estimate["rate"])

plot(qN, x, xlab = "Theoretical quantiles", ylab = "Empirical quantiles",
     main = "Flinders Street", pch = 16, col = "#8F250C", cex = 0.8)
points(qL, x, pch = 16, col = "#9cb380", cex = 0.8)
points(qG, x, pch = 16, col = "#124559", cex = 0.8)
abline(0, 1, lty = 2)

# QV Melbourne 
x <- sort(qv); n <- length(x); p <- (seq_len(n) - 0.5) / n
qN <- qnorm(p,  mean = fit_qv_norm$estimate["mean"],    sd = fit_qv_norm$estimate["sd"])
qL <- qlnorm(p, meanlog = fit_qv_logn$estimate["meanlog"], sdlog = fit_qv_logn$estimate["sdlog"])
qG <- qgamma(p, shape = fit_qv_gamma$estimate["shape"], rate = fit_qv_gamma$estimate["rate"])

plot(qN, x, xlab = "Theoretical quantiles", ylab = "Empirical quantiles",
     main = "QV Melbourne", pch = 16, col = "#8F250C", cex = 0.8)
points(qL, x, pch = 16, col = "#9cb380", cex = 0.8)
points(qG, x, pch = 16, col = "#124559", cex = 0.8)
abline(0, 1, lty = 2)

```



### Adequacy checks and assumptions

Adequacy was assessed with Q–Q plots. For Southern Cross, points lie close to the 45° line under a Normal reference; for Flinders and QV, log(count) aligns well with a Normal reference (supporting Lognormal on the count scale), with only small upper-tail deviations. Alternative count models (e.g., Poisson/Negative Binomial) were considered; Poisson was rejected due to over-dispersion, and although a Negative Binomial can also fit over-dispersed counts, the large counts make continuous approximations (Normal/Lognormal) adequate and visually superior here. Assumptions: observations are independent across days within a site; the fitted family (Normal or Lognormal) is a reasonable approximation for lunchtime variability.

## Task 2 - 90th Percentile Flow

### Purpose

The purpose of this task was to estimate the 90th percentile pedestrian flow for each crossing in order to check whether they meet the regulation that crossings should operate with smooth pedestrian flow 90 percent of the time. This means that on 90 percent of the days, the pedestrian count should be below a certain value so that congestion is avoided. The 90th percentile therefore represents the upper limit of typical daily usage and provides an important benchmark for infrastructure design. A crossing that can handle this level of flow without delays can be considered adequately sized

#### Approach 1: Sample Quantile

In the first approach, the 90th percentile was calculated directly from the observed data using the quantile() function. To account for sampling uncertainty, 95 percent bootstrap confidence intervals were constructed using 1,000 resamples. The bootstrap method was chosen because it does not assume any particular distributional form and is appropriate for moderately sized datasets.

```{r, sample_quantile}
pedestrian_long %>%
  group_by(crossing) %>%
  summarise(q90 = quantile(count, 0.9)) %>%
  kable(caption = "Younger vs Older: Average Per-capita Online Leisure Minutes",
        digits = 1, align = c("l","r")) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed", "responsive"),
    full_width = FALSE
  ) %>%
  row_spec(0, bold = TRUE, color = "#FFFFFF", background = "#598392") %>%
  column_spec(1, bold = TRUE, color = "#004D40")
  
# function to calculate 90th percentile
q90_fn <- function(data, i) quantile(data[i], 0.9)

# Flinders Street
fl_boot <- boot(pedestrian_df$flinders_street, q90_fn, R = 1000)
ci_fl <- boot.ci(fl_boot, type = "perc")

# Southern Cross
sc_boot <- boot(pedestrian_df$southern_cross, q90_fn, R = 1000)
ci_sc <- boot.ci(sc_boot, type = "perc")

# QV Melbourne
qv_boot <- boot(pedestrian_df$qv_melbourne, q90_fn, R = 1000)
ci_qv <- boot.ci(qv_boot, type = "perc")

sample_tbl <- tibble(
  crossing        = c("Flinders Street","Southern Cross","QV Melbourne"),
  q90_sample      = round(c(
    quantile(pedestrian_df$flinders_street, 0.9),
    quantile(pedestrian_df$southern_cross, 0.9),
    quantile(pedestrian_df$qv_melbourne, 0.9)
  ), 2),
  q90_sample_low  = round(c(ci_fl$percent[4], ci_sc$percent[4], ci_qv$percent[4]), 2),
  q90_sample_high = round(c(ci_fl$percent[5], ci_sc$percent[5], ci_qv$percent[5]), 2)
)

```

These results indicate that the busiest 10 percent of days have counts exceeding approximately 1,000 pedestrians per hour for the two station crossings and around 2,700 for QV Melbourne.

#### Approach 2: Model-Based Estimation

In the second approach, the fitted probability models from Task 1 were used to estimate theoretical 90th percentiles. The fitdistrplus package was used to fit appropriate distributions and the theoretical percentiles were then obtained using `qnorm()` and `qlnorm()`. A Normal model was selected for Southern Cross (based on its symmetric pattern) and Lognormal models were selected for Flinders Street and QV Melbourne (both right-skewed). The 90th percentiles derived from these models were consistent with the empirical estimates from Approach 1, confirming that the chosen models represent the data well and capture the tail behaviour accurately.

```{r, estimation}
# Compute the theoretical 90th percentile from each fitted model
q90_model_point <- list(
  fl = qlnorm(0.9, meanlog = fit_fl_logn$estimate["meanlog"], sdlog = fit_fl_logn$estimate["sdlog"]),
  sc = qnorm (0.9, mean    = fit_sc_norm$estimate["mean"],     sd    = fit_sc_norm$estimate["sd"]),
  qv = qlnorm(0.9, meanlog = fit_qv_logn$estimate["meanlog"], sdlog = fit_qv_logn$estimate["sdlog"])
)

q90_model_ci <- function(fit, dist, n, B = 1000) {
  boot_q90 <- replicate(B, {
    sim <- if (dist == "normal") {
      rnorm(n, mean = fit$estimate["mean"], sd = fit$estimate["sd"])
    } else {
      rlnorm(n, meanlog = fit$estimate["meanlog"], sdlog = fit$estimate["sdlog"])
    }
    quantile(sim, 0.9)
  })
  quantile(boot_q90, c(0.025, 0.975))
}

n_fl <- nrow(pedestrian_df)
n_sc <- nrow(pedestrian_df)
n_qv <- nrow(pedestrian_df)

ci_fl_m <- q90_model_ci(fit_fl_logn, "lognormal", n_fl)
ci_sc_m <- q90_model_ci(fit_sc_norm, "normal", n_sc)
ci_qv_m <- q90_model_ci(fit_qv_logn, "lognormal", n_qv)

model_tbl <- tibble(
  crossing       = c("Flinders Street","Southern Cross","QV Melbourne"),
  model          = c("lognormal","normal","lognormal"),
  q90_model      = round(c(q90_model_point$fl, q90_model_point$sc, q90_model_point$qv), 2),
  q90_model_low  = round(c(ci_fl_m[1], ci_sc_m[1], ci_qv_m[1]), 2),
  q90_model_high = round(c(ci_fl_m[2], ci_sc_m[2], ci_qv_m[2]), 2)
)

```

#### Discussion and Recommendation

```{r, task2_discussion}
final_tbl <- sample_tbl %>%
  left_join(model_tbl, by = "crossing")

final_tbl %>%
  kable(
    caption = "Younger vs Older: Average Per-capita Online Leisure Minutes",
    digits = 1,
    align = rep("c", ncol(final_tbl))  
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed", "responsive"),
    full_width = FALSE,
    position = "center"
  ) %>%
  row_spec(0, bold = TRUE, color = "#FFFFFF", background = "#598392") %>%
  column_spec(1, bold = TRUE, color = "#004D40")
```

Both the sample based and model based approaches produced very similar estimates of the 90th percentile, showing that the fitted models are appropriate for these datasets. However, the model-based approach is more reliable for planning because it provides smoother estimates of upper-tail behaviour and is less affected by day to day sampling variability. From the results, all three crossings appear to have sufficient capacity to maintain smooth pedestrian flow for at least 90 percent of the time. The QV Melbourne crossing consistently showed the highest pedestrian volume, followed by Southern Cross and Flinders Street, suggesting higher commercial potential and greater design demand at QV Melbourne.

## Task 3 - Engineering Design Comparison

The purpose of this analysis was to determine whether the Flinders Street and Southern Cross crossings can use the same design and materials. The engineers specified that the average difference in hourly pedestrian flow between the two locations should not exceed 80 people per hour for the design to be considered interchangeable.

To test this, a paired t-test was used to compare hourly pedestrian counts recorded simultaneously at both sites. This method was chosen because it accounts for shared conditions such as time of day and weather, which could affect pedestrian flow. A 95% confidence interval for the mean difference in pedestrian counts between Southern Cross and Flinders Street was calculated. The assumption of approximate normality required for the paired t-test was checked using a QQ plot of the differences. The points on the plot closely followed a straight line, confirming that the distribution of differences was approximately normal. Given the relatively large sample size of 97 paired observations, the Central Limit Theorem ensures that the sampling distribution of the mean difference is approximately normal, even if the underlying data are not perfectly so. Therefore, it is valid to use the CLT in this context, and the paired t-test provides a reliable confidence interval.
```{r}
df  <- na.omit(pedestrian_df[, c("southern_cross","flinders_street")])
sc  <- df$southern_cross
fl  <- df$flinders_street
d   <- sc - fl                 

# 95% CI for the mean difference 
tt <- t.test(sc, fl, paired = TRUE, conf.level = 0.95)
paired_tt_tbl <- tibble(
  Comparison = "Southern Cross − Flinders Street",
  Mean_Diff  = tt$estimate,
  `95% CI (Low)` = tt$conf.int[1],
  `95% CI (High)` = tt$conf.int[2],
  t_value = tt$statistic,
  df = tt$parameter,
  p_value = tt$p.value)

paired_tt_tbl %>%
  kable(
    caption = "Paired t-test: Southern Cross vs Flinders Street (95% confidence interval for mean difference)",
    digits = 2,
    align = rep("c", ncol(.))
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed", "responsive"),
    full_width = FALSE,
    position = "center"
  ) %>%
  row_spec(0, bold = TRUE, color = "#FFFFFF", background = "#598392") %>%
  column_spec(1, bold = TRUE, color = "#004D40")
```

```{r, task3_qq}
qqnorm(
  scale(d),
  main = "QQ of differences: Southern Cross − Flinders",
  pch  = 16,
  col  = adjustcolor("#3a405a", alpha.f = 0.5))

qqline(
  scale(d),
  col  = adjustcolor(cross_cols["Flinders Street"], alpha.f = 0.9), 
  lwd  = 2,
  lty  = 2)
```


The analysis found that Southern Cross had, on average, 48.75 more pedestrians per hour than Flinders Street, with a 95% confidence interval ranging from approximately 21 to 77 people per hour. This range lies entirely within the ±80 threshold defined by the engineering team. The QQ plot supported the reliability of the interval estimate by confirming that the differences were approximately normally distributed.

```{r, task3_plot}
ci <- tibble(
  estimate = tt$estimate, 
  lower = tt$conf.int[1],
  upper = tt$conf.int[2]
)

ggplot(ci) +
  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin = -80, ymax = 80),
            fill = cross_cols["QV Melbourne"], alpha = 0.15) +
  geom_errorbar(aes(x = 1, ymin = lower, ymax = upper),
                width = 0.08, linewidth = 1,
                colour = cross_cols["Southern Cross"]) +
  geom_point(aes(x = 1, y = estimate),
             size = 3, colour = cross_cols["Flinders Street"]) +
  geom_hline(yintercept = 0, linetype = "dashed",
             colour = "#444444") +
  coord_cartesian(ylim = c(-120, 120)) +
  scale_x_continuous(limits = c(0.5, 1.5), breaks = NULL) +
  labs(
    title = "95% CI for Mean Difference: Southern Cross − Flinders (paired)",
    subtitle = "Shaded zone = ±80 people/hour (design tolerance)",
    y = "Difference in mean counts (SC − FL)", x = NULL
  ) +
  theme_minimal(base_size = 13) +
  theme(
    plot.title = element_text(colour = cross_cols["Southern Cross"], face = "bold"),
    plot.subtitle = element_text(colour = cross_cols["Flinders Street"])
  )

```

Based on these results, we can be 95% confident that the true mean difference in pedestrian counts between the two crossings is less than 80 people per hour. This indicates that while Southern Cross tends to be slightly busier, the difference is not large enough to justify separate designs. Therefore, the engineers can safely use the same design and materials for both crossings, achieving cost savings without compromising on performance or pedestrian safety. The analysis and results are statistically sound and provide a clear, evidence-based recommendation for the engineering team.

## Task 4 - Marketing Value Comparison

### Purpose

The purpose of this task was to estimate and compare the average hourly pedestrian counts at three major Melbourne crossings: Southern Cross, Flinders Street, and QV Melbourne. The aim was to help the marketing team evaluate which locations offer the highest advertising value for billboard placements. Pedestrian flow directly represents potential audience exposure, so understanding the mean number of people passing through each site is crucial for assessing advertising reach. Constructing 95 percent confidence intervals around these means provides insight into both the central tendency and the uncertainty of the estimates. This information allows the marketing team to make data-driven decisions about which sites are most likely to deliver the best visibility and return on investment for advertising campaigns.

```{r, results='hide', task4_tt}
# Southern Cross
tt_sc <- t.test(pedestrian_df$southern_cross, conf.level = 0.95)

# Flinders Street
tt_fl <- t.test(pedestrian_df$flinders_street, conf.level = 0.95)

# QV
tt_qv <- t.test(pedestrian_df$qv, conf.level = 0.95)

```

```{r, task4_results}
tt_table <- tibble(
  Location = c("Southern Cross", "Flinders Street", "QV Melbourne"),
  Mean     = c(tt_sc$estimate, tt_fl$estimate, tt_qv$estimate),
  `95% CI (Low)`  = c(tt_sc$conf.int[1], tt_fl$conf.int[1], tt_qv$conf.int[1]),
  `95% CI (High)` = c(tt_sc$conf.int[2], tt_fl$conf.int[2], tt_qv$conf.int[2]))

tt_table %>%
  kable(
    caption = "Mean pedestrian counts with 95% confidence intervals (one-sample t-tests)",
    digits = 1,
    align = rep("c", ncol(.))
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed", "responsive"),
    full_width = FALSE,
    position = "center"
  ) %>%
  row_spec(0, bold = TRUE, color = "#FFFFFF", background = "#598392") %>%
  column_spec(1, bold = TRUE, color = "#004D40")
```

### Results Table

The table shows that QV Melbourne has a much higher mean pedestrian count than the other two locations, while Southern Cross and Flinders Street have similar average flows.

### Visualisation

A lolipop plot with 95 percent confidence intervals (@fig-trafficCI) was created to visually compare the mean pedestrian traffic at each site. Each dot represents the estimated mean, and the vertical error bars show the confidence intervals for those estimates. The intervals for Southern Cross and Flinders Street overlap considerably, suggesting that their mean pedestrian flows are statistically similar. The interval for QV Melbourne, however, is clearly separated from the others and positioned much higher, indicating that this site consistently experiences greater pedestrian movement. The visualisation provides an immediate and intuitive comparison, helping the marketing team to identify the most valuable advertising locations based on both the magnitude and the reliability of foot traffic estimates.

```{r, task4_plot}
ggplot(tt_table, aes(x = Location, y = Mean, color = Location)) +
  geom_point(size = 4) +
  geom_errorbar(aes(ymin = `95% CI (Low)`, ymax = `95% CI (High)`), width = 0.15, linewidth = 1) +
  geom_segment(aes(xend = Location, y = 0, yend = Mean), color = "grey80") +
  scale_color_manual(values = cross_cols) +
  labs(
    title = "Pedestrian Traffic with 95% Confidence Intervals",
    y = "Mean pedestrians per hour",
    x = "Location"
  ) +
  theme_minimal(base_size = 14) +
  theme(legend.position = "none")
```

### Interpretation

QV Melbourne recorded the highest average pedestrian flow of about 2300 people per hour, approximately double that of Southern Cross and Flinders Street. This indicates that QV Melbourne offers the strongest advertising potential and the highest likelihood of exposure for billboard campaigns. Southern Cross and Flinders Street have mean flows below 1000 people per hour and are therefore suited for mid-level advertising opportunities.

The confidence intervals were computed using one-sample t-tests, which are appropriate when the population standard deviation is unknown. With 97 observations at each location, the Central Limit Theorem ensures that the sampling distribution of the mean is approximately normal, even if the underlying data are slightly skewed. The narrower intervals at the two station crossings indicate stable and predictable pedestrian traffic, while the wider interval at QV Melbourne reflects greater variability that naturally accompanies higher volumes.

## Task 5 - Billboard Revenue Estimate

### Purpose

The purpose of this task was to estimate the expected billboard revenue that could be earned under the bank’s proposed offer at each of the three pedestrian crossings: Southern Cross, Flinders Street, and QV Melbourne. According to the offer, the bank pays a base amount of \$10,000 for each billboard and provides a bonus of \$5,000 multiplied by the proportion of days when pedestrian counts exceed 1,000 people per hour. Estimating this proportion and its associated uncertainty allows the marketing and finance teams to forecast expected earnings and to identify which sites provide the greatest commercial value. The results help quantify how consistent and reliable each location is in attracting high pedestrian traffic and therefore advertising exposure

## Method

The analysis was conducted in two main steps. First, for each crossing, the proportion of days where pedestrian counts were greater than 1,000 per hour was calculated empirically from the observed data. This proportion represents the probability θ = P(count \> 1,000), indicating how frequently the threshold was surpassed.

Second, a 95 percent confidence interval for this probability was computed using the Wilson score interval from the prop.test() function in R. The Wilson method was chosen because it performs well even for small or extreme proportions, and it includes a continuity correction to produce accurate bounds when the probability is close to 0 or 1.

The expected revenue was then calculated using the formula:

::: {.callout-note icon="calculator" title="Formula for expected billboard revenue"}
Revenue = 10,000 + 5,000 × θ
:::

The same transformation was applied to the lower and upper bounds of the confidence intervals for θ to obtain the revenue intervals. This approach allowed both the average expected revenue and its uncertainty to be represented for each location.

```{r, task5_calc}
THRESH  <- 1000
BASE    <- 10000
BONUS   <- 5000
LEVEL   <- 0.95

# Binomial proportion + Wilson CI via prop.test (built-in)
rev_ci <- pedestrian_long %>%
  mutate(over_1000 = count > THRESH) %>%
  group_by(crossing) %>%
  summarise(
    n = n(),
    k = sum(over_1000),
    .groups = "drop"
  ) %>%
  rowwise() %>%
  mutate(
    pt   = list(prop.test(k, n, conf.level = LEVEL, correct = TRUE)),
    phat = pt$estimate[[1]],
    lo_p = pt$conf.int[1],
    hi_p = pt$conf.int[2],
    rev_hat = BASE + BONUS * phat,
    rev_lo  = BASE + BONUS * lo_p,
    rev_hi  = BASE + BONUS * hi_p
  ) %>%
  ungroup() 

rev_ci <- rev_ci[, c("crossing","n", "k", "phat", "lo_p", "hi_p", "rev_hat", "rev_lo", "rev_hi")]

rev_ci %>%
  kable(
    caption = "Younger vs Older: Average Per-capita Online Leisure Minutes",
    digits = 1,
    align = rep("c", ncol(final_tbl))  
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed", "responsive"),
    full_width = FALSE,
    position = "center"
  ) %>%
  row_spec(0, bold = TRUE, color = "#FFFFFF", background = "#598392") %>%
  column_spec(1, bold = TRUE, color = "#004D40")


```

## Results Table

The results show that QV Melbourne consistently exceeds 1,000 pedestrians per hour on every observed day, resulting in the maximum possible revenue under the bank’s payment structure. Southern Cross and Flinders Street earn lower but similar expected revenues, reflecting their smaller and more variable pedestrian volumes.

## Visualisation

A horizontal bar chart with 95 percent confidence intervals (@fig-task5) was produced to visually compare the expected revenues for the three locations. Each bar represents the average expected revenue, and the error bars display the range of uncertainty based on the Wilson confidence intervals. The plot shows that QV Melbourne achieves the highest and most stable revenue at approximately \$15,000, with its interval tightly concentrated at the maximum bonus limit. Southern Cross and Flinders Street appear close together on the chart, indicating similar revenue potential of around \$11,000. The relatively narrow intervals for these two sites suggest consistent performance, but their mean revenues are notably below that of QV Melbourne. This visual comparison provides an intuitive summary of which sites are most profitable and how confident we can be in those estimates.

```{r, task5_plot}
ggplot(rev_ci, aes(x = reorder(crossing, rev_hat), y = rev_hat)) +
  geom_col(aes(fill = crossing), width = 0.6) +
  geom_errorbar(aes(ymin = rev_lo, ymax = rev_hi), width = 0.2) +
  geom_text(aes(label = scales::dollar(rev_hat, accuracy = 1)),
            vjust = -0.5, size = 3.5) +
  scale_fill_manual(values = cross_cols) +
  scale_y_continuous(
    labels = scales::label_dollar(prefix = "$", accuracy = 1),
    expand = expansion(mult = c(0, 0.08))
  ) +
  coord_flip() +
  labs(
    title = "Expected revenue under bank offer",
    x = NULL, y = "Expected revenue"
  ) +
  theme_minimal(base_size = 12) +
  theme(legend.position = "none")
```

## Interpretation

The analysis indicates that QV Melbourne would generate the highest expected revenue, reaching the full amount of \$15,000, since it consistently experiences pedestrian flows above the 1,000-person threshold. Southern Cross and Flinders Street would yield similar revenues, both around \$11,000, due to their lower proportions of days above the threshold.

These results suggest that QV Melbourne is by far the most attractive location for billboard placement under the bank’s offer, offering both the highest guaranteed visibility and the lowest revenue uncertainty. The two station crossings provide steady but moderate exposure and may still be valuable for lower-cost or secondary advertising campaigns. Overall, the use of the Wilson interval method provides reliable and interpretable confidence bounds that clearly distinguish the three locations in terms of expected financial `returns.`

## References

1. Angelo Canty and Brian Ripley (2024). boot: Bootstrap R (S-Plus) Functions. R package version 1.3-30.

2. Davison, A. C. & Hinkley, D. V. (1997) Bootstrap Methods and Their Applications. Cambridge University Press, Cambridge.
  ISBN 0-521-57391-2
  
3. Venables, W. N. & Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth Edition. Springer, New York. ISBN
  0-387-95457-0
  
4. Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M,
  Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo
  K, Yutani H (2019). “Welcome to the tidyverse.” _Journal of Open Source Software_, *4*(43), 1686.
  doi:10.21105/joss.01686 <https://doi.org/10.21105/joss.01686>.

5. Xie Y (2025). _knitr: A General-Purpose Package for Dynamic Report Generation in R_. R package version 1.50,
  <https://yihui.org/knitr/>.

6. Yihui Xie (2015) Dynamic Documents with R and knitr. 2nd edition. Chapman and Hall/CRC. ISBN 978-1498716963

7. Yihui Xie (2014) knitr: A Comprehensive Tool for Reproducible Research in R. In Victoria Stodden, Friedrich Leisch and
  Roger D. Peng, editors, Implementing Reproducible Computational Research. Chapman and Hall/CRC. ISBN 978-1466561595

8. Zhu H (2024). _kableExtra: Construct Complex Table with 'kable' and Pipe Syntax_. R package version 1.4.0,
  <https://CRAN.R-project.org/package=kableExtra>.
  
9. Robinson D, Hayes A, Couch S (2024). _broom: Convert Statistical Objects into Tidy Tibbles_. R package version 1.0.6,
  <https://CRAN.R-project.org/package=broom>.
  
10. Wickham H, Henry L (2025). _purrr: Functional Programming Tools_. R package version 1.0.4,
  <https://CRAN.R-project.org/package=purrr>.
  
11.  Wickham H, Pedersen T, Seidel D (2023). _scales: Scale Functions for Visualization_. R package version 1.3.0,
  <https://CRAN.R-project.org/package=scales>.

12. *Master Maximum Likelihood Estimation in R: A Step-by-Step Guide.* (2018, July 16). Analytics Vidhya. <https://www.analyticsvidhya.com/blog/2018/07/introductory-guide-maximum-likelihood-estimation-case-study-r/>

13. Bevans, R. (2020, March 26). *Akaike Information Criterion | When & How to Use It (Example).* Scribbr. <https://www.scribbr.com/statistics/akaike-information-criterion/>

14. *How to Perform Bootstrapping in R* | R-bloggers. (2022, December 17). <https://www.r-bloggers.com/2022/12/how-to-perform-bootstrapping-in-r/>

15. *boot.ci function—RDocumentation.*, from <https://www.rdocumentation.org/packages/boot/versions/1.3-32/topics/boot.ci?utm_source=chatgpt.com>




