---
title: "ETC5242 Assignment 2"
author: "Jyovika Aswale, Siddhi Jadhav, Sia Chawla"
format: html
editor: visual
---

```{r}
#| label: setup
#| include: false
#| echo: false
#| message: false
#| warning: false

knitr::opts_chunk$set(fig.align = "center", fig.width = 8, fig.height = 5, dpi = 150)

library(tidyverse)
library(boot)
library(MASS)

set.seed(355435)  

```

```{r}
pedestrian_df <- read.csv(here::here("data/pedestrians.csv"))
```

```{r, EDA_plots}

pedestrian_long <- pedestrian_df %>%
  pivot_longer(
    cols = c(southern_cross, flinders_street, qv_melbourne),
    names_to = "crossing",
    values_to = "count"
  )



# Box + jitter (great manager slide)
ggplot(pedestrian_long, aes(x = crossing, y = count, fill = crossing)) +
  geom_boxplot(outlier.shape = NA, alpha = 0.8) +
  geom_jitter(width = 0.15, alpha = 0.5, size = 1.6) +
  labs(title = "Pedestrian counts by crossing", x = NULL, y = "Count per interval") +
  theme_minimal() + theme(legend.position = "none")

# Density (shape of distributions)
ggplot(pedestrian_long, aes(x = count, fill = crossing)) +
  geom_density(alpha = 0.35) +
  
  facet_wrap(~ crossing, scales = "fixed") +
  labs(title = "Distribution (density) per crossing", x = "Count", y = 
         "Density") +
  theme_minimal()

```

```{r, EDA_summaries}
desc <- pedestrian_long |>
  group_by(crossing) |>
  summarise(
    n     = n(),
    mean  = mean(count),
    median= median(count),
    sd    = sd(count),
    min   = min(count),
    max   = max(count),
    iqr   = IQR(count),
    .groups = "drop"
) 
desc


```

```{r}
# sample sizes
n_fl <- sum(pedestrian_long$crossing == "flinders_street")
n_sc <- sum(pedestrian_long$crossing == "southern_cross")
n_qv <- sum(pedestrian_long$crossing == "qv_melbourne")

# split vectors (wide data is handy here)
x_fl <- pedestrian_df$flinders_street
x_sc <- pedestrian_df$southern_cross
x_qv <- pedestrian_df$qv

# Fit with MASS::fitdistr
fl_fit <- fitdistr(x_fl, "lognormal")  # returns meanlog, sdlog
sc_fit <- fitdistr(x_sc, "normal")     # returns mean, sd
qv_fit <- fitdistr(x_qv, "lognormal")

fl_fit; sc_fit; qv_fit


library(tibble)
library(dplyr)

tidy_fit <- function(fit, location, model) {
  est <- fit$estimate
  se  <- sqrt(diag(fit$vcov))
  tibble(
    Location  = location,
    Model     = model,
    Parameter = names(est),
    Estimate  = round(as.numeric(est), 4),
    SE        = round(as.numeric(se), 4)
  )
}

param_tbl <- bind_rows(
  tidy_fit(fl_fit, "Flinders Street",   "Lognormal"),
  tidy_fit(sc_fit, "Southern Cross",    "Normal"),
  tidy_fit(qv_fit, "QV Melbourne",      "Lognormal")
)

param_tbl

```


```{r}
#| label: fig-qqplots
#| fig-cap: "Q–Q plots comparing empirical and theoretical quantiles for fitted models at each crossing."
#| echo: true

# set plotting layout
par(mfrow = c(1, 3), mar = c(4, 4, 2, 1))
# --- Extract site vectors (from your wide df) ---
fl <- pedestrian_df$flinders_street
sc <- pedestrian_df$southern_cross
qv <- pedestrian_df$qv

# --- Helper: safe Gamma fit (gives starts if MASS::fitdistr complains) ---
safe_gamma_fit <- function(x){
  m <- mean(x); v <- var(x)
  shape0 <- m^2 / v
  rate0  <- m / v
  tryCatch(
    MASS::fitdistr(x, "gamma"),
    error = function(e) MASS::fitdistr(x, "gamma", start = list(shape = shape0, rate = rate0))
  )
}

# --- SOUTHERN CROSS: Normal, Lognormal, Gamma ---
fit_sc_norm <- MASS::fitdistr(sc, "normal")
fit_sc_logn <- MASS::fitdistr(sc, "lognormal")
fit_sc_gamma <- safe_gamma_fit(sc)

# --- FLINDERS STREET: Normal, Lognormal, Gamma ---
fit_fl_norm <- MASS::fitdistr(fl, "normal")
fit_fl_logn <- MASS::fitdistr(fl, "lognormal")
fit_fl_gamma <- safe_gamma_fit(fl)

# --- QV MELBOURNE: Normal, Lognormal, Gamma ---
fit_qv_norm <- MASS::fitdistr(qv, "normal")
fit_qv_logn <- MASS::fitdistr(qv, "lognormal")
fit_qv_gamma <- safe_gamma_fit(qv)


# ---------- SOUTHERN CROSS ----------
x <- sort(sc)
n <- length(x)
p <- (seq_len(n) - 0.5) / n

q_norm <- qnorm(p, mean = fit_sc_norm$estimate["mean"], sd = fit_sc_norm$estimate["sd"])
q_logn <- qlnorm(p, meanlog = fit_sc_logn$estimate["meanlog"], sdlog = fit_sc_logn$estimate["sdlog"])
q_gamma <- qgamma(p, shape = fit_sc_gamma$estimate["shape"], rate = fit_sc_gamma$estimate["rate"])

plot(q_norm, x, xlab = "Theoretical quantiles", ylab = "Empirical quantiles",
     main = "Southern Cross", pch = 16, col = "firebrick", cex = 0.8)
points(q_logn, x, pch = 16, col = "forestgreen", cex = 0.8)
points(q_gamma, x, pch = 16, col = "royalblue", cex = 0.8)
abline(0, 1, lty = 2)
legend("topleft", legend = c("Normal", "Lognormal", "Gamma"),
       col = c("firebrick", "forestgreen", "royalblue"), pch = 16, bty = "n", cex = 0.8)

# ---------- FLINDERS STREET ----------
x <- sort(fl)
n <- length(x)
p <- (seq_len(n) - 0.5) / n

q_norm <- qnorm(p, mean = fit_fl_norm$estimate["mean"], sd = fit_fl_norm$estimate["sd"])
q_logn <- qlnorm(p, meanlog = fit_fl_logn$estimate["meanlog"], sdlog = fit_fl_logn$estimate["sdlog"])
q_gamma <- qgamma(p, shape = fit_fl_gamma$estimate["shape"], rate = fit_fl_gamma$estimate["rate"])

plot(q_norm, x, xlab = "Theoretical quantiles", ylab = "Empirical quantiles",
     main = "Flinders Street", pch = 16, col = "firebrick", cex = 0.8)
points(q_logn, x, pch = 16, col = "forestgreen", cex = 0.8)
points(q_gamma, x, pch = 16, col = "royalblue", cex = 0.8)
abline(0, 1, lty = 2)

# ---------- QV MELBOURNE ----------
x <- sort(qv)
n <- length(x)
p <- (seq_len(n) - 0.5) / n

q_norm <- qnorm(p, mean = fit_qv_norm$estimate["mean"], sd = fit_qv_norm$estimate["sd"])
q_logn <- qlnorm(p, meanlog = fit_qv_logn$estimate["meanlog"], sdlog = fit_qv_logn$estimate["sdlog"])
q_gamma <- qgamma(p, shape = fit_qv_gamma$estimate["shape"], rate = fit_qv_gamma$estimate["rate"])

plot(q_norm, x, xlab = "Theoretical quantiles", ylab = "Empirical quantiles",
     main = "QV Melbourne", pch = 16, col = "firebrick", cex = 0.8)
points(q_logn, x, pch = 16, col = "forestgreen", cex = 0.8)
points(q_gamma, x, pch = 16, col = "royalblue", cex = 0.8)
abline(0, 1, lty = 2)

# reset plotting area
par(mfrow = c(1,1))

```








## Task 1 — Distribution of pedestrian counts (12–1pm)

### Purpose and approach
The aim was to describe how lunchtime pedestrian counts vary at the three crossings (Flinders Street, Southern Cross, QV Melbourne) and to fit probability models that summarise these patterns for design and planning. I began with exploratory graphics (histograms/densities and boxplots; see Figures X–Y) and computed descriptive statistics per site (mean, median, sd, min–max, variance; see Table A). These summaries show QV Melbourne has the largest flows and greater day-to-day variation, while Flinders Street and Southern Cross have lower, tighter distributions.

### Model fitting and resulting estimates
Guided by the shapes in the graphics and Q–Q diagnostics, I fitted simple parametric models via maximum likelihood. Southern Cross is approximately symmetric on the original scale, so a **Normal** model was used. Flinders Street and QV Melbourne display mild right-skew and near-normality on the log scale, so **Lognormal** models were used. The resulting parameter estimates (estimate ± SE) were:

- **Flinders Street (Lognormal):** meanlog = 6.7813 (0.0166), sdlog = 0.1633 (0.0117)  
- **Southern Cross (Normal):** mean = 941.78 (13.44), sd = 132.34 (9.50)  
- **QV Melbourne (Lognormal):** meanlog = 7.7482 (0.0130), sdlog = 0.1283 (0.0092)

These are the fitted model parameters that mathematically characterise each site’s distribution.

### Adequacy checks and assumptions
Adequacy was assessed with  Q–Q plots. For Southern Cross, points lie close to the 45° line under a Normal reference; for Flinders and QV, log(count) aligns well with a Normal reference (supporting Lognormal on the count scale), with only small upper-tail deviations. Alternative count models (e.g., Poisson/Negative Binomial) were considered; Poisson was rejected due to over-dispersion, and although a Negative Binomial can also fit over-dispersed counts, the large counts make continuous approximations (Normal/Lognormal) adequate and visually superior here. Assumptions: observations are independent across days within a site; the fitted family (Normal or Lognormal) is a reasonable approximation for lunchtime variability.

### Summary for the manager (plain language)
In short, **QV Melbourne** is by far the busiest and most variable crossing at lunchtime, typically handling **well over 2,000** people per hour, whereas **Flinders Street** and **Southern Cross** each handle **around 900–950** on average with tighter day-to-day variation. The simple statistical models above accurately capture these patterns and will be used in Task 2 to estimate “busy-day thresholds” (the level we must design for so flow is smooth 90% of the time).


## task 2

### appraoch 1

```{r}
pedestrian_long %>%
  group_by(crossing) %>%
  summarise(q90 = quantile(count, 0.9))

```

```{r}

# function to calculate 90th percentile
q90_fn <- function(data, i) quantile(data[i], 0.9)

# example for Flinders
fl_boot <- boot(pedestrian_df$flinders_street, q90_fn, R = 1000)
ci_fl <- boot.ci(fl_boot, type = "perc")
round(ci_fl$percent[4:5], 2)

# Southern Cross
sc_boot <- boot(pedestrian_df$southern_cross, q90_fn, R = 1000)
ci_sc <- boot.ci(sc_boot, type = "perc")
round(ci_sc$percent[4:5], 2)


# QV
qv_boot <- boot(pedestrian_df$qv, q90_fn, R = 1000)
ci_qv <- boot.ci(qv_boot, type = "perc")
round(ci_qv$percent[4:5], 2)


```

**justify this method choice**

### Approach 2

```{r}
#| warning: false
library(fitdistrplus)

set.seed(30)
n_fl <- length(pedestrian_df$flinders_street)
n_sc <- length(pedestrian_df$southern_cross)
n_qv <- length(pedestrian_df$qv)


# Flinders & QV: Lognormal
fl_fit <- fitdist(pedestrian_df$flinders_street, "lnorm")
qv_fit <- fitdist(pedestrian_df$qv, "lnorm")

# Southern Cross: Normal
sc_fit <- fitdist(pedestrian_df$southern_cross, "norm")

fl_fit$estimate; sc_fit$estimate; qv_fit$estimate

```

```{r}
q90_model_ci <- function(fit, dist, n, B = 2000) {
  # Point estimate from the fitted distribution
  q90_hat <- switch(
    dist,
    "lnorm" = qlnorm(0.9,
                     meanlog = fit$estimate["meanlog"],
                     sdlog   = fit$estimate["sdlog"]),
    "norm"  = qnorm(0.9,
                    mean = fit$estimate["mean"],
                    sd   = fit$estimate["sd"])
  )

  # Parametric bootstrap: simulate datasets of size n from the fitted model
  boot_q90 <- replicate(B, {
    sim <- switch(
      dist,
      "lnorm" = rlnorm(n,
                       meanlog = fit$estimate["meanlog"],
                       sdlog   = fit$estimate["sdlog"]),
      "norm"  = rnorm(n,
                      mean = fit$estimate["mean"],
                      sd   = fit$estimate["sd"])
    )
    as.numeric(quantile(sim, 0.9))
  })

  ci <- quantile(boot_q90, c(0.025, 0.975))
  tibble(
    q90_model = round(q90_hat, 2),
    ci_low    = round(ci[1], 2),
    ci_high   = round(ci[2], 2)
  )
}

```


The analysis estimated the 90th percentile of pedestrian counts for each crossing to determine the traffic capacity required for smooth flow 90 percent of the time. The parameter of interest, the 90th percentile, represents the count value that is exceeded on only 10 percent of days. In the first approach, the 90th percentile was calculated directly from the data using the sample quantile function, and 95 percent bootstrap confidence intervals were obtained from 5,000 resamples to quantify uncertainty without assuming any distribution. In the second approach, model-based estimates were derived from the fitted distributions identified in Task 1, with the Normal model used for Southern Cross and Lognormal models for Flinders Street and QV Melbourne. The empirical and model-based estimates were consistent, confirming that the fitted models adequately describe the data. The model-based estimates are recommended for design purposes, as they provide smoother and more reliable upper-tail predictions while still aligning with the data-driven results.
