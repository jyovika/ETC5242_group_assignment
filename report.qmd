---
title: "ETC5242 Assignment 2"
author: "Jyovika Aswale, Siddhi Jadhav, Sia Chawla"
format: html
editor: visual
---

```{r}
#| label: setup
#| include: false
#| echo: false
#| message: false
#| warning: false

knitr::opts_chunk$set(fig.align = "center", fig.width = 8, fig.height = 5, dpi = 150)

library(tidyverse)
library(boot)
library(MASS)

set.seed(355435)  

```

```{r}
pedestrian_df <- read.csv(here::here("data/pedestrians.csv"))
```

```{r, EDA_plots}

pedestrian_long <- pedestrian_df %>%
  pivot_longer(
    cols = c(southern_cross, flinders_street, qv_melbourne),
    names_to = "crossing",
    values_to = "count"
  )



# Box + jitter (great manager slide)
ggplot(pedestrian_long, aes(x = crossing, y = count, fill = crossing)) +
  geom_boxplot(outlier.shape = NA, alpha = 0.8) +
  geom_jitter(width = 0.15, alpha = 0.5, size = 1.6) +
  labs(title = "Pedestrian counts by crossing", x = NULL, y = "Count per interval") +
  theme_minimal() + theme(legend.position = "none")

# Density (shape of distributions)
ggplot(pedestrian_long, aes(x = count, fill = crossing)) +
  geom_density(alpha = 0.35) +
  
  facet_wrap(~ crossing, scales = "fixed") +
  labs(title = "Distribution (density) per crossing", x = "Count", y = 
         "Density") +
  theme_minimal()

```

```{r, EDA_summaries}
desc <- pedestrian_long |>
  group_by(crossing) |>
  summarise(
    n     = n(),
    mean  = mean(count),
    median= median(count),
    sd    = sd(count),
    min   = min(count),
    max   = max(count),
    iqr   = IQR(count),
    .groups = "drop"
) 
desc


```

```{r}
# sample sizes
n_fl <- sum(pedestrian_long$crossing == "flinders_street")
n_sc <- sum(pedestrian_long$crossing == "southern_cross")
n_qv <- sum(pedestrian_long$crossing == "qv_melbourne")

# split vectors (wide data is handy here)
x_fl <- pedestrian_df$flinders_street
x_sc <- pedestrian_df$southern_cross
x_qv <- pedestrian_df$qv

# Fit with MASS::fitdistr
fl_fit <- fitdistr(x_fl, "lognormal")  # returns meanlog, sdlog
sc_fit <- fitdistr(x_sc, "normal")     # returns mean, sd
qv_fit <- fitdistr(x_qv, "lognormal")

fl_fit; sc_fit; qv_fit


library(tibble)
library(dplyr)

tidy_fit <- function(fit, location, model) {
  est <- fit$estimate
  se  <- sqrt(diag(fit$vcov))
  tibble(
    Location  = location,
    Model     = model,
    Parameter = names(est),
    Estimate  = round(as.numeric(est), 4),
    SE        = round(as.numeric(se), 4)
  )
}

param_tbl <- bind_rows(
  tidy_fit(fl_fit, "Flinders Street",   "Lognormal"),
  tidy_fit(sc_fit, "Southern Cross",    "Normal"),
  tidy_fit(qv_fit, "QV Melbourne",      "Lognormal")
)

param_tbl

```


```{r}
#| label: fig-qqplots
#| fig-cap: "Q–Q plots comparing empirical and theoretical quantiles for fitted models at each crossing."
#| echo: true

# set plotting layout
par(mfrow = c(1, 3), mar = c(4, 4, 2, 1))
# --- Extract site vectors (from your wide df) ---
fl <- pedestrian_df$flinders_street
sc <- pedestrian_df$southern_cross
qv <- pedestrian_df$qv

# --- Helper: safe Gamma fit (gives starts if MASS::fitdistr complains) ---
safe_gamma_fit <- function(x){
  m <- mean(x); v <- var(x)
  shape0 <- m^2 / v
  rate0  <- m / v
  tryCatch(
    MASS::fitdistr(x, "gamma"),
    error = function(e) MASS::fitdistr(x, "gamma", start = list(shape = shape0, rate = rate0))
  )
}

# --- SOUTHERN CROSS: Normal, Lognormal, Gamma ---
fit_sc_norm <- MASS::fitdistr(sc, "normal")
fit_sc_logn <- MASS::fitdistr(sc, "lognormal")
fit_sc_gamma <- safe_gamma_fit(sc)

# --- FLINDERS STREET: Normal, Lognormal, Gamma ---
fit_fl_norm <- MASS::fitdistr(fl, "normal")
fit_fl_logn <- MASS::fitdistr(fl, "lognormal")
fit_fl_gamma <- safe_gamma_fit(fl)

# --- QV MELBOURNE: Normal, Lognormal, Gamma ---
fit_qv_norm <- MASS::fitdistr(qv, "normal")
fit_qv_logn <- MASS::fitdistr(qv, "lognormal")
fit_qv_gamma <- safe_gamma_fit(qv)


# ---------- SOUTHERN CROSS ----------
x <- sort(sc)
n <- length(x)
p <- (seq_len(n) - 0.5) / n

q_norm <- qnorm(p, mean = fit_sc_norm$estimate["mean"], sd = fit_sc_norm$estimate["sd"])
q_logn <- qlnorm(p, meanlog = fit_sc_logn$estimate["meanlog"], sdlog = fit_sc_logn$estimate["sdlog"])
q_gamma <- qgamma(p, shape = fit_sc_gamma$estimate["shape"], rate = fit_sc_gamma$estimate["rate"])

plot(q_norm, x, xlab = "Theoretical quantiles", ylab = "Empirical quantiles",
     main = "Southern Cross", pch = 16, col = "firebrick", cex = 0.8)
points(q_logn, x, pch = 16, col = "forestgreen", cex = 0.8)
points(q_gamma, x, pch = 16, col = "royalblue", cex = 0.8)
abline(0, 1, lty = 2)
legend("topleft", legend = c("Normal", "Lognormal", "Gamma"),
       col = c("firebrick", "forestgreen", "royalblue"), pch = 16, bty = "n", cex = 0.8)

# ---------- FLINDERS STREET ----------
x <- sort(fl)
n <- length(x)
p <- (seq_len(n) - 0.5) / n

q_norm <- qnorm(p, mean = fit_fl_norm$estimate["mean"], sd = fit_fl_norm$estimate["sd"])
q_logn <- qlnorm(p, meanlog = fit_fl_logn$estimate["meanlog"], sdlog = fit_fl_logn$estimate["sdlog"])
q_gamma <- qgamma(p, shape = fit_fl_gamma$estimate["shape"], rate = fit_fl_gamma$estimate["rate"])

plot(q_norm, x, xlab = "Theoretical quantiles", ylab = "Empirical quantiles",
     main = "Flinders Street", pch = 16, col = "firebrick", cex = 0.8)
points(q_logn, x, pch = 16, col = "forestgreen", cex = 0.8)
points(q_gamma, x, pch = 16, col = "royalblue", cex = 0.8)
abline(0, 1, lty = 2)

# ---------- QV MELBOURNE ----------
x <- sort(qv)
n <- length(x)
p <- (seq_len(n) - 0.5) / n

q_norm <- qnorm(p, mean = fit_qv_norm$estimate["mean"], sd = fit_qv_norm$estimate["sd"])
q_logn <- qlnorm(p, meanlog = fit_qv_logn$estimate["meanlog"], sdlog = fit_qv_logn$estimate["sdlog"])
q_gamma <- qgamma(p, shape = fit_qv_gamma$estimate["shape"], rate = fit_qv_gamma$estimate["rate"])

plot(q_norm, x, xlab = "Theoretical quantiles", ylab = "Empirical quantiles",
     main = "QV Melbourne", pch = 16, col = "firebrick", cex = 0.8)
points(q_logn, x, pch = 16, col = "forestgreen", cex = 0.8)
points(q_gamma, x, pch = 16, col = "royalblue", cex = 0.8)
abline(0, 1, lty = 2)

# reset plotting area
par(mfrow = c(1,1))

```








## Task 1 — Distribution of pedestrian counts (12–1pm)

### Purpose and approach
The aim was to describe how lunchtime pedestrian counts vary at the three crossings (Flinders Street, Southern Cross, QV Melbourne) and to fit probability models that summarise these patterns for design and planning. I began with exploratory graphics (histograms/densities and boxplots; see Figures X–Y) and computed descriptive statistics per site (mean, median, sd, min–max, variance; see Table A). These summaries show QV Melbourne has the largest flows and greater day-to-day variation, while Flinders Street and Southern Cross have lower, tighter distributions.

### Model fitting and resulting estimates
Guided by the shapes in the graphics and Q–Q diagnostics, I fitted simple parametric models via maximum likelihood. Southern Cross is approximately symmetric on the original scale, so a **Normal** model was used. Flinders Street and QV Melbourne display mild right-skew and near-normality on the log scale, so **Lognormal** models were used. The resulting parameter estimates (estimate ± SE) were:

- **Flinders Street (Lognormal):** meanlog = 6.7813 (0.0166), sdlog = 0.1633 (0.0117)  
- **Southern Cross (Normal):** mean = 941.78 (13.44), sd = 132.34 (9.50)  
- **QV Melbourne (Lognormal):** meanlog = 7.7482 (0.0130), sdlog = 0.1283 (0.0092)

These are the fitted model parameters that mathematically characterise each site’s distribution.

### Adequacy checks and assumptions
Adequacy was assessed with  Q–Q plots. For Southern Cross, points lie close to the 45° line under a Normal reference; for Flinders and QV, log(count) aligns well with a Normal reference (supporting Lognormal on the count scale), with only small upper-tail deviations. Alternative count models (e.g., Poisson/Negative Binomial) were considered; Poisson was rejected due to over-dispersion, and although a Negative Binomial can also fit over-dispersed counts, the large counts make continuous approximations (Normal/Lognormal) adequate and visually superior here. Assumptions: observations are independent across days within a site; the fitted family (Normal or Lognormal) is a reasonable approximation for lunchtime variability.

### Summary for the manager (plain language)
In short, **QV Melbourne** is by far the busiest and most variable crossing at lunchtime, typically handling **well over 2,000** people per hour, whereas **Flinders Street** and **Southern Cross** each handle **around 900–950** on average with tighter day-to-day variation. The simple statistical models above accurately capture these patterns and will be used in Task 2 to estimate “busy-day thresholds” (the level we must design for so flow is smooth 90% of the time).


## task 2

### appraoch 1

```{r}
pedestrian_long %>%
  group_by(crossing) %>%
  summarise(q90 = quantile(count, 0.9))

```

```{r}

# function to calculate 90th percentile
q90_fn <- function(data, i) quantile(data[i], 0.9)

# example for Flinders
fl_boot <- boot(pedestrian_df$flinders_street, q90_fn, R = 1000)
ci_fl <- boot.ci(fl_boot, type = "perc")
round(ci_fl$percent[4:5], 2)

# Southern Cross
sc_boot <- boot(pedestrian_df$southern_cross, q90_fn, R = 1000)
ci_sc <- boot.ci(sc_boot, type = "perc")
round(ci_sc$percent[4:5], 2)


# QV
qv_boot <- boot(pedestrian_df$qv, q90_fn, R = 1000)
ci_qv <- boot.ci(qv_boot, type = "perc")
round(ci_qv$percent[4:5], 2)

sample_tbl <- tibble(
  Location  = c("Flinders Street", "Southern Cross", "QV Melbourne"),
  q90_sample= round(c(quantile(fl, 0.9), quantile(sc, 0.9), quantile(qv, 0.9)), 2),
  ci_low    = round(c(ci_fl$percent[4], ci_sc$percent[4], ci_qv$percent[4]), 2),
  ci_high   = round(c(ci_fl$percent[5], ci_sc$percent[5], ci_qv$percent[5]), 2)
)

sample_tbl


```

Why this method?
Quantiles don’t have a simple small-sample SE formula. The bootstrap re-samples your data many times, recomputes the 90th percentile, and uses the empirical spread for a 95% CI, without assuming any distribution.

**justify this methods choice**

### Approach 2




```{r}
# Compute the theoretical 90th percentile from each fitted model
q90_model_point <- list(
  fl = qlnorm(0.9, meanlog = fl_fit$estimate["meanlog"], sdlog = fl_fit$estimate["sdlog"]),
  sc = qnorm(0.9,  mean = sc_fit$estimate["mean"],        sd   = sc_fit$estimate["sd"]),
  qv = qlnorm(0.9, meanlog = qv_fit$estimate["meanlog"], sdlog = qv_fit$estimate["sdlog"])
)

# Parametric bootstrap for model-based CI
q90_model_ci <- function(fit, dist, n, B = 1000) {
  boot_q90 <- replicate(B, {
    sim <- if (dist == "normal") {
      rnorm(n, mean = fit$estimate["mean"], sd = fit$estimate["sd"])
    } else { # lognormal
      rlnorm(n, meanlog = fit$estimate["meanlog"], sdlog = fit$estimate["sdlog"])
    }
    quantile(sim, 0.9)
  })
  quantile(boot_q90, c(0.025, 0.975))
}

ci_fl_m <- q90_model_ci(fl_fit, "lognormal", n_fl)
ci_sc_m <- q90_model_ci(sc_fit, "normal",    n_sc)
ci_qv_m <- q90_model_ci(qv_fit, "lognormal", n_qv)

model_tbl <- tibble(
  Location   = c("Flinders Street", "Southern Cross", "QV Melbourne"),
  Model      = c("Lognormal", "Normal", "Lognormal"),
  q90_model  = round(c(q90_model_point$fl, q90_model_point$sc, q90_model_point$qv), 2),
  ci_low     = round(c(ci_fl_m[1], ci_sc_m[1], ci_qv_m[1]), 2),
  ci_high    = round(c(ci_fl_m[2], ci_sc_m[2], ci_qv_m[2]), 2)
)

model_tbl

```



```{r}
final_tbl <- sample_tbl %>%
  rename(q90_sample_low = ci_low, q90_sample_high = ci_high) %>%
  left_join(model_tbl %>%
              rename(q90_model_low = ci_low, q90_model_high = ci_high),
            by = "Location")

final_tbl

```

What to write (justification & conclusion)

Why two methods?

Sample quantile is assumption-free and directly reflects the data; bootstrap CIs are standard for quantiles.

Model-based leverages the structure you established in Task 1; when the model fits well (per Q–Q plots), it can be more precise (slightly narrower CIs).

Assumptions used:

Days are independent within a crossing.

For Approach 2 only: the fitted family is adequate (Southern Cross ~ Normal; Flinders & QV ~ Lognormal).

How to interpret results (template):

Flinders & Southern Cross typically have 90th percentiles just above ~1100 pedestrians/hour.

QV Melbourne’s 90th percentile is around ~2700–2800, substantially higher.

The two approaches give similar point estimates; model-based CIs are a bit narrower, consistent with greater efficiency under correct model specification.

For design, recommend using the model-based estimate (with its CI) if the Task-1 diagnostics were satisfactory; otherwise default to the sample quantile.
 



The analysis estimated the 90th percentile of pedestrian counts for each crossing to determine the traffic capacity required for smooth flow 90 percent of the time. The parameter of interest, the 90th percentile, represents the count value that is exceeded on only 10 percent of days. In the first approach, the 90th percentile was calculated directly from the data using the sample quantile function, and 95 percent bootstrap confidence intervals were obtained from 1,000 resamples to quantify uncertainty without assuming any distribution. In the second approach, model-based estimates were derived from the fitted distributions identified in Task 1, with the Normal model used for Southern Cross and Lognormal models for Flinders Street and QV Melbourne. The empirical and model-based estimates were consistent, confirming that the fitted models adequately describe the data. The model-based estimates are recommended for design purposes, as they provide smoother and more reliable upper-tail predictions while still aligning with the data-driven results.



The purpose of this analysis was to estimate the 90th percentile of pedestrian counts at each of the three crossings—Flinders Street, Southern Cross, and QV Melbourne. This value represents the pedestrian volume that is exceeded only 10 percent of the time, and is therefore an important measure for assessing whether crossings are adequately designed to maintain smooth pedestrian flow under busy conditions.

Two approaches were used to estimate the 90th percentile. The first approach used the sample quantile, calculated directly from the observed data. The second approach used fitted probability models (identified in Task 1) to derive model-based quantile estimates. For both approaches, 95 percent confidence intervals were computed to reflect the uncertainty in estimation.

In the first approach, the 90th percentile was computed using the quantile() function, and a non-parametric bootstrap with 1000 resamples was used to estimate 95 percent confidence intervals. The bootstrap method is appropriate here because quantiles do not have simple standard errors, and the bootstrap empirically estimates variability by resampling the data many times. The results were as follows: Flinders Street – 90th percentile of 1060.8 (95% CI: 1038.6 to 1157.1); Southern Cross – 1090.0 (95% CI: 1057.2 to 1128.0); and QV Melbourne – 2704.0 (95% CI: 2624.0 to 2939.0). These results suggest that QV Melbourne experiences much heavier pedestrian traffic during its busiest periods, while Flinders Street and Southern Cross have relatively similar upper-end counts.

In the second approach, theoretical 90th percentiles were calculated using the distributions fitted in Task 1—Lognormal for Flinders Street and QV Melbourne, and Normal for Southern Cross. The corresponding formulas were applied to estimate the 90th percentile under each model, and a parametric bootstrap with 1000 simulated samples was used to compute 95 percent confidence intervals. The model-based estimates were: Flinders Street – 1086.3 (95% CI: 1028.9 to 1145.1); Southern Cross – 1111.4 (95% CI: 1066.1 to 1152.8); and QV Melbourne – 2731.7 (95% CI: 2613.3 to 2847.2).

Both approaches produced similar point estimates, indicating that the fitted models were appropriate for the data. The model-based confidence intervals were slightly narrower than those from the sample quantiles, reflecting higher precision when the assumed model fits well. Overall, the results show that QV Melbourne consistently records the highest pedestrian counts, with its 90th percentile around 2700–2730 people, compared with roughly 1060–1110 for Flinders Street and Southern Cross.

The bootstrap confidence interval approach was chosen because it provides a flexible and reliable way to assess uncertainty without relying on large-sample normal approximations. The model-based approach is more efficient when the distributional assumptions are correct, while the sample quantile method remains robust if those assumptions are violated.

For practical recommendations, the model-based estimates should be used when planning or evaluating pedestrian infrastructure, as they account for natural variability and provide more stable estimates. However, if future data deviate from the fitted Normal or Lognormal models, the simpler sample quantile method would still yield dependable results.

In summary, the 90th percentile analysis confirms that all three crossings handle their regular pedestrian traffic effectively, but QV Melbourne operates at significantly higher capacity levels and may require additional infrastructure consideration to maintain smooth flow during peak hours.
